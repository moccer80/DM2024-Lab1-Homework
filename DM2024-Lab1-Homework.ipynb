{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:張禕紘\n",
    "\n",
    "Student ID:312707043\n",
    "\n",
    "GitHub ID:161785685"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data). The dataset contains a `sentiment` and `comment` columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'. Read the specificiations of the dataset for background details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/didiersalazar/DM2024-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
"# take home exercise",
      "",
      "## exercise 2",
      "X[X[\"category\"]==1]",
      "X.iloc[-1:,:]",
      "X.columns",
      "X.index",
      "X.info()",
      "",
      "## exercise 5",
      "import numpy as np",
      "NA_dict = [{ 'id': 'A', 'missing_example': np.nan },",
      "           { 'id': 'B'                    },",
      "           { 'id': 'C', 'missing_example': 'NaN'  },",
      "           { 'id': 'D', 'missing_example': 'None' },",
      "           { 'id': 'E', 'missing_example':  None  },",
      "           { 'id': 'F', 'missing_example': ''     }]",
      "NA_df = pd.DataFrame(NA_dict, columns = ['id','missing_example'])",
      "print(NA_df)",
      "NA_df['missing_example'].isnull()",
      "# index[2] and [3] are seen as str",
      "# index[5] is seen as empty str, not None",
      "",
      "## exercise 6",
      "# X_sample is selected randomly from X ,n=1000 means you choose 1000 sample from X",
      "",
      "## exercise 8",
      "a=X[\"category_name\"].value_counts()",
      "b=X_sample[\"category_name\"].value_counts()",
      "c=pd.DataFrame({\"x\":a,\"x_s\":b})",
      "c.plot(kind = \"bar\",rot=0,fontsize=10)",
      "plt.show()",
      "",
      "## exercise 10",
      "for i in range(50):",
      "    if X_counts[4,i] == 1:",
      "        print(count_vect.get_feature_names_out()[i])",
      "row, col = X_counts[0:5,0:100].nonzero()  ",
      "for i in range(len(col)):",
      "    print(f\"in row {row[i]},feature {count_vect.get_feature_names_out()[col[i]]} appear\")",
      "",
      "## exercise 11",
      "x=list(np.random.choice(count_vect.get_feature_names_out(),20,replace=False))",
      "xt=[]",
      "for i in x:",
      "    xt.append(list(count_vect.get_feature_names_out()).index(i))",
      "y=[]",
      "y=list(np.random.random_integers(0,len(X.index),size=20))",
      "z =X_counts[y, :][:, xt].toarray()",
      "z=pd.DataFrame(z,columns=x,index=y)",
      "sns.heatmap(z,cmap=\"PuRd\",vmin=0, annot=True)",
      "",
      "## exercise 12",
      "import plotly.graph_objects as go",
      "fig = go.Figure([go.Bar(x=count_vect.get_feature_names_out()[:300], y=term_frequencies_temp[0,:300])])",
      "fig.show()",
      "",
      "## exercise 13",
      "fig = go.Figure([go.Bar(x=count_vect.get_feature_names_out()[:50], y=term_frequencies_temp[0,:50])])",
      "fig.show()",
      "",
      "## exercise 14",
      "mat=pd.DataFrame(term_frequencies_temp[0,:300],index=count_vect.get_feature_names_out()[:300],columns=[\"freq\"])",
      "mat=mat.sort_values(by=\"freq\",ascending=False)",
      "sns.barplot(data=mat,x=mat.index,y=\"freq\")",
      "plt.ylim(0,mat[\"freq\"].max())",
      "plt.xticks([])",
      "plt.show()",
      "",
      "## exercise 15",
      "mat[\"log_freq\"]=np.log(mat[\"freq\"])",
      "mat=mat.sort_values(by=\"log_freq\",ascending=False)",
      "sns.barplot(data=mat,x=mat.index,y=\"log_freq\")",
      "plt.ylim(0,mat[\"log_freq\"].max())",
      "plt.xticks([])",
      "plt.show()",
      "",
      "## exercise 16",
      "for i in range(len(categories)):",
      "    t=pd.DataFrame(filtered_term_document_dfs[categories[i]].sum(),columns=[\"freq\"])",
      "    t.sort_values(by=\"freq\",ascending=False,inplace=True)",
      "    print(f\"category {categories[i]} appear most frequenctly 15 words\")",
      "    print(t[\"freq\"].nlargest(15))",
      "",
      "## exercise 17",
      "from PAMI.frequentPattern.basic import FPGrowth as alg",
      "for i in range(3,10,3):",
      "    minSup=3",
      "    obj3 = alg.FPGrowth(iFile='td_freq_db_comp_graphics.csv', minSup=minSup)",
      "    obj3.mine()",
      "    frequentPatternsDF_comp_graphics= obj3.getPatternsAsDataFrame()",
      "    print('Total No of patterns: ' + str(len(frequentPatternsDF_comp_graphics))) #print the total number of patterns",
      "    print('Runtime: ' + str(obj3.getRuntime())) #measure the runtime",
      "    for j in range(1,4):",
      "        k=500*j",
      "        print(frequentPatternsDF_comp_graphics.nlargest(k,columns=\"Support\"))",
      "",
      "## exercise 18",
      "X_pca_aug_exer = PCA(n_components=3).fit_transform(augmented_df.values)",
      "X_tsne_aug_exer = TSNE(n_components=3).fit_transform(augmented_df.values)",
      "X_umap_aug_exer = umap.UMAP(n_components=3).fit_transform(augmented_df.values)",
      "import plotly.express as px",
      "fig_pca = px.scatter_3d(X_pca_aug_exer, x=0, y=1, z=2, color=X['category_name'], title='PCA',labels={\"0\":\"x\",\"1\":\"y\",\"2\":\"z\"})",
      "fig_pca.update_traces(marker=dict(",
      "    size=2,  ",
      "    symbol='circle',  ",
      "    opacity=0.8,  ",
      "    line=dict(width=2)  ",
      "))",
      "fig_pca.show()",
      "fig_tsne = px.scatter_3d(X_tsne_aug_exer, x=0, y=1, z=2, color=X['category_name'], title='tsne',labels={\"0\":\"x\",\"1\":\"y\",\"2\":\"z\"})",
      "fig_tsne.update_traces(marker=dict(",
      "    size=2,  ",
      "    symbol='circle',  ",
      "    opacity=0.8,  ",
      "    line=dict(width=2)  ",
      "))",
      "fig_tsne.show()",
      "fig_umap = px.scatter_3d(X_umap_aug_exer, x=0, y=1, z=2, color=X['category_name'], title='tsne',labels={\"0\":\"x\",\"1\":\"y\",\"2\":\"z\"})",
      "fig_umap.update_traces(marker=dict(",
      "    size=2,  ",
      "    symbol='circle',  ",
      "    opacity=0.8,  ",
      "    line=dict(width=2)  ",
      "))",
      "fig_umap.show()",
      "# 透過三維視圖，可以讓區分類別變得更容易",
      "",
      "",
      "## exercise 19 ",
      "mlb.fit_transform(X['category']).tolist()"

   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}



